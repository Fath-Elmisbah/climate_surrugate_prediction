{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b80fd389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from keras.layers.normalization.layer_normalization import *\n",
    "from keras.layers.normalization.batch_normalization import *\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow_probability as tfp\n",
    "import clif\n",
    "import clif.visualization as cviz\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as kb\n",
    "import tensorflow_addons as tfa\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad3802f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-neuralnetwork\n",
      "  Downloading scikit-neuralnetwork-0.7.tar.gz (33 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.17 in /home/misbah/anaconda3/envs/proj-env/lib/python3.6/site-packages (from scikit-neuralnetwork) (0.24.2)\n",
      "Collecting Theano>=0.8\n",
      "  Downloading Theano-1.0.5.tar.gz (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Lasagne>=0.1\n",
      "  Downloading Lasagne-0.1.tar.gz (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/misbah/anaconda3/envs/proj-env/lib/python3.6/site-packages (from Lasagne>=0.1->scikit-neuralnetwork) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/misbah/anaconda3/envs/proj-env/lib/python3.6/site-packages (from scikit-learn>=0.17->scikit-neuralnetwork) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/misbah/anaconda3/envs/proj-env/lib/python3.6/site-packages (from scikit-learn>=0.17->scikit-neuralnetwork) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/misbah/anaconda3/envs/proj-env/lib/python3.6/site-packages (from scikit-learn>=0.17->scikit-neuralnetwork) (3.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/misbah/anaconda3/envs/proj-env/lib/python3.6/site-packages (from Theano>=0.8->scikit-neuralnetwork) (1.15.0)\n",
      "Building wheels for collected packages: scikit-neuralnetwork, Lasagne, Theano\n",
      "  Building wheel for scikit-neuralnetwork (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-neuralnetwork: filename=scikit_neuralnetwork-0.7-py3-none-any.whl size=41698 sha256=5fdab674fc12e85b4416716f10bc99ec8f8771f210af5ae3815b0acfd5158d01\n",
      "  Stored in directory: /home/misbah/.cache/pip/wheels/32/68/e9/8d101abd9783af57c4fc7d0f757551f1e8c78ee1d522417cfd\n",
      "  Building wheel for Lasagne (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Lasagne: filename=Lasagne-0.1-py3-none-any.whl size=79283 sha256=4e4c2e994233cb6358170d565c159e8eb75fbe8b5b422a84f44e289db1cf9b63\n",
      "  Stored in directory: /home/misbah/.cache/pip/wheels/b9/77/1a/ea5fb64846a17e27d33c290668ea73be7038d7ce643d3882c0\n",
      "  Building wheel for Theano (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Theano: filename=Theano-1.0.5-py3-none-any.whl size=2668111 sha256=d2adb9136fffb60790902983f3abc4cac0b40bee4b1cd0c5f67bc5acc77debda\n",
      "  Stored in directory: /home/misbah/.cache/pip/wheels/7f/80/57/2970ddd2e4961d84fb0d58196f4965deead046d29e08693e73\n",
      "Successfully built scikit-neuralnetwork Lasagne Theano\n",
      "Installing collected packages: Theano, Lasagne, scikit-neuralnetwork\n",
      "Successfully installed Lasagne-0.1 Theano-1.0.5 scikit-neuralnetwork-0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-neuralnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af6663d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '../../../src/data'\n",
      "/home/misbah/Desktop/CS 229/Project/icme-xplore-bayes-spring22/src/data\n"
     ]
    }
   ],
   "source": [
    "cd ../../../src/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59eac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5d9d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the data set and load it via chunks (dask) for efficient handling (optional)\n",
    "season = 'DJF' # season\n",
    "dataset = xr.open_mfdataset(f\"../data/lat_lon_10yr_24x48_{season}.nc\", chunks={'n': 1})\n",
    "Y = dataset['SWCF']\n",
    "X = dataset['lhs']\n",
    "X_bnds = dataset['lhs_bnds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dcdc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the observation or reference data\n",
    "dataset_obs = xr.open_mfdataset(f\"../data/lat_lon_24x48_{season}_obs.nc\", chunks=1)\n",
    "Y_obs = dataset_obs['SWCF']\n",
    "Y_scalar=Y_obs.values.var()\n",
    "Y_shift=Y_obs.values.mean()\n",
    "Y_np = (Y.values-Y_obs.values)/np.sqrt(Y_scalar)\n",
    "Y_np = np.array([Yi.flatten() for Yi in Y_np])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e59be699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names of the feature coordinates:\n",
      " ['ice_sed_ai' 'clubb_c1' 'clubb_gamma_coef' 'zmconv_tau' 'zmconv_dmpdz']\n",
      "List of upper and lower bounds:\n",
      " [array([ 350., 1400.]), array([1., 5.]), array([0.1, 0.5]), array([ 1800., 14400.]), array([-0.002 , -0.0001])]\n",
      "Range of scaled features:(-1.000,0.998)\n"
     ]
    }
   ],
   "source": [
    "X_np = X.values\n",
    "feature_coords = dataset['x'].values\n",
    "print(\"names of the feature coordinates:\\n\",feature_coords)\n",
    "\n",
    "feature_bounds = dataset['lhs_bnds'].values\n",
    "print(\"List of upper and lower bounds:\\n\", list(feature_bounds))\n",
    "\n",
    "from tesuract.preprocessing import DomainScaler\n",
    "feature_transform = DomainScaler(\n",
    "                dim=X_np.shape[1],\n",
    "                input_range=list(feature_bounds),\n",
    "                output_range=(-1,1),\n",
    "                )\n",
    "X_s = feature_transform.fit_transform(X_np)\n",
    "print(\"Range of scaled features:({0:.3f},{1:.3f})\".format(X_s.min(), X_s.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d6e1635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xarray.core.dataarray.DataArray"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a56c4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.random.choice(250,50)\n",
    "X_test=X[index]\n",
    "Y_test=Y[index]\n",
    "Index_x_bool=np.array([True]*250)\n",
    "Index_x_bool[index]=False\n",
    "X_train=X[Index_x_bool]\n",
    "Y_train=Y[Index_x_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23cd7d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/10\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 3179.7544WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 3186.5408 - val_loss: 3137.4231\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 3177.2131 - val_loss: 3133.1655\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 3173.8284 - val_loss: 3131.6196\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 3172.8716 - val_loss: 3130.9912\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 3172.1365 - val_loss: 3129.5510\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 3170.9602 - val_loss: 3128.9250\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 3170.3716 - val_loss: 3128.7598\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 3170.1792 - val_loss: 3128.5903\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 3170.0540 - val_loss: 3128.5439\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 3170.0146 - val_loss: 3128.4419\n"
     ]
    }
   ],
   "source": [
    "# Re-label the data to X and Y\n",
    "X = X_s.copy()\n",
    "Y = Y_np.copy()\n",
    "\n",
    "# Autoencoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "latent_dim = 64\n",
    "\n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim, data_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim   \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "          layers.Flatten(),\n",
    "          layers.Dense(latent_dim, activation='relu'),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "          layers.Dense(data_dim[0]*data_dim[1], activation='sigmoid'),\n",
    "          layers.Reshape(data_dim)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim, Y_train.shape[1:]) \n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder.fit(Y_train, Y_train,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(Y_test, Y_test))\n",
    "\n",
    "encoded_imgs = autoencoder.encoder(Y_test.values).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "170f2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train2=Yhat[Index_x_bool]\n",
    "Y_test2=Yhat[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2381fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_actual,y_pred):\n",
    "    custom_loss=kb.square(y_actual-y_pred)\n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5d99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_PCA(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.BatchNormalization(input_shape=(5,)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(\n",
    "    units=hp.Int(\"units\", min_value=20, max_value=100, step=20),\n",
    "    activation=hp.Choice(\"activation\", [\"relu\", \"tanh\",\"leaky_relu\"]))\n",
    "    )\n",
    "#     if hp.Boolean(\"dropout\"):\n",
    "#         model.add(keras.layers.Dropout(rate=0.25))\n",
    "        \n",
    "    model.add(keras.layers.Dense(\n",
    "    units=hp.Int(\"units2\", min_value=20, max_value=100, step=20),\n",
    "    activation=hp.Choice(\"activation\", [\"relu\", \"tanh\",\"leaky_relu\"]))\n",
    "    )\n",
    "    model.add(keras.layers.Dense(\n",
    "    units=hp.Int(\"units3\", min_value=20, max_value=100, step=20),\n",
    "    activation=hp.Choice(\"activation\", \"leaky_relu\"))\n",
    "    )\n",
    "    model.add(keras.layers.Dense(16, activation='leaky_relu'))\n",
    "    hp_learning_rate=hp.Choice(\"learning_rate\",values=[1e-2,1e-3,1e-4])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=hp_learning_rate),loss=custom_loss,\n",
    "             metrics=[ tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcbeecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model_PCA,\n",
    "    objective='val_loss',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    overwrite=True,\n",
    "    directory='trials',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93a1c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 02s]\n",
      "val_loss: 1.432041883468628\n",
      "\n",
      "Best val_loss So Far: 1.0402294397354126\n",
      "Total elapsed time: 00h 00m 38s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, Y_train2, epochs=50, validation_data=(X_test, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b929e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 40)                240       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                336       \n",
      "=================================================================\n",
      "Total params: 3,056\n",
      "Trainable params: 3,046\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models()[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d05764c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    }
   ],
   "source": [
    "Y_Pred=best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca5603a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_orig_test=pca.inverse_transform(Y_Pred)\n",
    "Y_test2=Y[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7152b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=np.mean(np.square(Y_orig_test-Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1896578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse=np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f85a1602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse is: 0.021786647 rmse: 0.147603\n"
     ]
    }
   ],
   "source": [
    "print(\"mse is:\",mse,\"rmse:\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d197377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(best_model, to_file='large_model_plot.png', show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "763d51a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cdb301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:proj-env] *",
   "language": "python",
   "name": "conda-env-proj-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
