{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66095c59",
   "metadata": {},
   "source": [
    "# Quick recap\n",
    "\n",
    "To get started, we need to make sure we have the right packages. The packages for surrogate construction and plotting is `tesuract` and `clif`. `tesuract` can be downloaded using `pip install tesuract` or getting the source from [here](https://github.com/kennychowdhary/tesuract). `clif` is a private repo and you should have got an invitation to be a collaborator. The link is [here](https://github.com/kennychowdhary/clif). We will use `clif` for plotting and some preprocessing transforms, but it is not required. \n",
    "\n",
    "These packages, and other will require the following third party libraries:\n",
    "\n",
    "```\n",
    "xarray\n",
    "dask\n",
    "numpy \n",
    "matplotlib\n",
    "netcdf4\n",
    "scikit-learn\n",
    "cartopy (optional)\n",
    "```\n",
    "\n",
    "Once you have all those packages installed, we can begin loading and plotting the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c3893",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "\n",
    "The data is saved as a netcdf file, which is similar to a pandas style data frame. We will load is as an xarray data set which will allow us to load a whole bunch of meta data along with the numpy arrays. For example, the data sets we will be looking at are latitude-longitude fields. If we were to import the data as a pure numpy array, we would lose the latitude or longitude indices associated with each row and column of the data set. xarray allows us to store that information. It is a mix between pandas and numpy, and it becoming increasingly utilized in the climate world. \n",
    "\n",
    "The data sets are broken up by seasons (DJF - December, January, February). So for DJF/ winter, we have two separate `*.nc` files or netcdf files. `lat_lon_10yr_24x48_DJF.nc` contains simulation data from 250 independent runs of E3SM. It contains different climatologies or outputs of the model such as shortwave and longwave cloud forcing, and total precipitation. Let's see how to load this data and what it looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec230941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559867a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/felixmeng/CME291/icme-xplore-bayes-spring22/src/data\n"
     ]
    }
   ],
   "source": [
    "cd /Users/felixmeng/CME291/icme-xplore-bayes-spring22/src/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4200be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the data set and load it via chunks (dask) for efficient handling (optional)\n",
    "season = 'DJF' # season\n",
    "dataset = xr.open_mfdataset(f\"../data/lat_lon_10yr_24x48_{season}.nc\", chunks={'n': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab05189",
   "metadata": {},
   "source": [
    "For our purposes, we will only need a few of these variables. The variables of interest will be \n",
    "\n",
    "```\n",
    "SWCF (shortwave cloud forcing W/m^2)\n",
    "LWCF (longwave cloud forcing W/m^2)\n",
    "PRECT (total precipitation m/day)\n",
    "area (latitude/ longitude area weights of grids, not equally spaced)\n",
    "lhs (Latin hypercuve sampling tuning parameters/ feature space samples X)\n",
    "lhs_bnds (bounds on feature space parameters - for re-scaling)\n",
    "```\n",
    "\n",
    "Let's extract the SWCF variable and the LHS points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728b9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the training data for SWCF\n",
    "Y = dataset['SWCF']\n",
    "X = dataset['lhs']\n",
    "X_bnds = dataset['lhs_bnds']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c1c50",
   "metadata": {},
   "source": [
    "Now that we extracted the training data, let's explore its shape. The first dimension is always the sampling dimension, as standard in ML. Thus, each row of the dataset corresponds to a different sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a8f42",
   "metadata": {},
   "source": [
    "$X$ is of size $250 \\times 5$ corresponding to $250$ training samples and $5$-dimensional feature space. Each row represents a different set of parameters and the corresponding E3SM model simulation output is given in $Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687c2db",
   "metadata": {},
   "source": [
    "$Y$ is of size $250 \\times 24 \\times 48$, thus each sample of $Y$ is a $24\\times48$ *image* or spatially varying latitude-longitude field. As you can see above, $Y$ is a three dimensional tensor (it's actually 4 but we ignore the time), with latitude and longitude coordinates. \n",
    "\n",
    "$(X,Y)$ represent input/ output pairs of the E3SM climate model. We will use this data to creat a regression model that maps/ interpolates $X$ to $Y$. We do this so we can calibrate the climate model, i.e., we want to find the optimal input value such that $Y$ matches some observed climate data. That is, \n",
    "\n",
    "$$x^* = \\arg \\min_{x} \\|Y_{\\mathrm{E^3SM}}(x) - Y_{\\mathrm{obs}} \\| $$\n",
    "\n",
    "Since E3SM is too computationally expensive, instead, we solve \n",
    "\n",
    "$$x^* = \\arg \\min_{x} \\|Y_{\\mathrm{surrogate}}(x) - Y_{\\mathrm{obs}} \\| $$, \n",
    "\n",
    "where we replace the full model with a machine learning surrogate. This optimization can be solved quickly given that the surrogate is differentiable and easy to evaluate. \n",
    "\n",
    "This observed data can be loaded from the corresponding netcdf file from the same data directory.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcaa2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the observation or reference data\n",
    "dataset_obs = xr.open_mfdataset(f\"../data/lat_lon_24x48_{season}_obs.nc\", chunks={'n': 1})\n",
    "Y_obs = dataset_obs['SWCF']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c31e80",
   "metadata": {},
   "source": [
    "## Creating a dummy predictor\n",
    "\n",
    "Let's create our first surrogate which maps $X \\mapsto Y$. The standard baseline or starting point would be to use a mean approximation as a model. Of course, this mean predictor has no dependence on $X$ but if it performs better than a more complex model, then it is a problem. So we always compute the dummy predictor as a default. \n",
    "\n",
    "We can use sklearn's dummy regressor to create such a model quite easily. For simplicity, it will help to flatten each of the $24\\times 48$ output data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "412cb154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 1152)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract numpy array from the xarray.DataArray\n",
    "Y_np = Y.values\n",
    "Y_np = np.array([Yi.flatten() for Yi in Y_np])\n",
    "Y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c6bf966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also, extract the feature matrix as a numpy array\n",
    "X_np = X.values\n",
    "X_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194a391",
   "metadata": {},
   "source": [
    "If you take a look at $X$, it contains vastly different scales. For better performance, we will do some feature scaling and transform the input data to a standard input. We will use tesuract to perform this transformation. To do this we need to extract the bounds from the dataset and then perform a feature transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abcc3707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names of the feature coordinates:\n",
      " ['ice_sed_ai' 'clubb_c1' 'clubb_gamma_coef' 'zmconv_tau' 'zmconv_dmpdz']\n",
      "List of upper and lower bounds:\n",
      " [array([ 350., 1400.]), array([1., 5.]), array([0.1, 0.5]), array([ 1800., 14400.]), array([-0.002 , -0.0001])]\n",
      "Range of scaled features:(-1.000,0.998)\n"
     ]
    }
   ],
   "source": [
    "feature_coords = dataset['x'].values\n",
    "print(\"names of the feature coordinates:\\n\",feature_coords)\n",
    "\n",
    "feature_bounds = dataset['lhs_bnds'].values\n",
    "print(\"List of upper and lower bounds:\\n\", list(feature_bounds))\n",
    "\n",
    "from tesuract.preprocessing import DomainScaler\n",
    "feature_transform = DomainScaler(\n",
    "                dim=X_np.shape[1],\n",
    "                input_range=list(feature_bounds),\n",
    "                output_range=(-1,1),\n",
    "                )\n",
    "X_s = feature_transform.fit_transform(X_np)\n",
    "print(\"Range of scaled features:({0:.3f},{1:.3f})\".format(X_s.min(), X_s.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee70533",
   "metadata": {},
   "source": [
    "## Establishing a baseline\n",
    "\n",
    "Try creating a surrogate four different ways and computing the cross validation score (5-fold cross validation score) for each of the regressors. Note that the regressors below have the ability to create a multi-target model (we have 1,152 targets which correspond to 24x48 latitude and longitude points). Not all regressors can perform a multi-target regression. \n",
    "\n",
    "Once a baseline is established, we can start experimenting with different metrics, hyper-parameter optimization, and we can try our first fully connect neural network. We can initially try sklearn's basic MLP model, which can handle multi-target data, but then we will move to keras since it can handle convolutional nets and most custom solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d67653",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yr/3yyj3s2j7ln9rr38ny17_0_m0000gn/T/ipykernel_25602/1486158733.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdummyreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "X = X_s.copy()\n",
    "Y = Y_np.copy()\n",
    "\n",
    "# calculate the cross validation score as well\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Now we can create a dummy predictor mapping X -> Y\n",
    "from sklearn.dummy import DummyRegressor\n",
    "dummyreg = DummyRegressor(strategy='mean')\n",
    "cv_scores = cross_val_score(model,X,Y,scoring='r2')\n",
    "print(cv_scores.mean())\n",
    "\n",
    "# try k nearest neighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3eaf7",
   "metadata": {},
   "source": [
    "### Creating a custom estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a82e98",
   "metadata": {},
   "source": [
    "We can create a custom estimator, e.g., from keras, and have it be compatible with sklearn's API simply by wrapping it using the BaseEstimator class. This is optional, but I am putting some pseudo code to explain how this can work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45ff4b2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yr/3yyj3s2j7ln9rr38ny17_0_m0000gn/T/ipykernel_25602/4016519396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mKerasNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "class KerasNeuralNetwork(BaseEstimator):\n",
    "    def __init__(self,nlayers=2,depth=[100]):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y=None,**fit_params):\n",
    "        '''\n",
    "        create a sequentil neural network\n",
    "        add layers and activations\n",
    "        model = Sequential(...)\n",
    "        model.fit(X,y)\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        model.predict(X)\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee29b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
